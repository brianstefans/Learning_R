{
  "hash": "71b6271268683af80855dbdcd706930d",
  "result": {
    "engine": "knitr",
    "markdown": "# Module 1 — Data Preparation & Standardization\n\n## Learning Outcomes\n\nBy the end of this module learners will be able to:\n\n-   Explain the difference between normalization and standardization and when to use each.\n-   Apply scaling methods in R (`scale()`, `caret::preProcess`) and create min-max and robust scaling.\n-   Detect and treat missing values and common outliers.\n-   Prepare data for PCA and clustering (center, scale, and check assumptions).\n-   Write small self-check tests to verify preprocessing steps.\n\n------------------------------------------------------------------------\n\n## 1. Why preprocessing matters\n\nMany algorithms (like K-means, PCA, distance-based methods) assume features are on comparable scales. Preprocessing ensures that units and magnitudes don’t distort the analysis.\n\n\n::: {.cell}\n\n:::\n\n\n------------------------------------------------------------------------\n\n## 2. Data types and missing values\n\n### Inspecting structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n```\n\n\n:::\n:::\n\n\n### Handling missing values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\niris_miss <- iris \nidx <- sample(seq_len(nrow(iris_miss)), size = floor(0.05 * nrow(iris_miss) ))\niris_miss$Sepal.Length[idx] <- NA\nsum(is.na(iris_miss$Sepal.Length))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7\n```\n\n\n:::\n\n```{.r .cell-code}\niris_drop <- na.omit(iris_miss)\niris_meanimp <- iris_miss\niris_meanimp$Sepal.Length[is.na(iris_meanimp$Sepal.Length)] <- mean(iris_meanimp$Sepal.Length, na.rm = TRUE)\n\n#iris_miss %>% mutate(Sepal.Length = tidyr::replace_na(Sepal.Length,mean(iris_miss$Sepal.Length,na.rm = T))) %>% head()\n\n# preProc <- preProcess(iris_miss[,1:4], method = c(\"knnImpute\"))\n# iris_knnimp <- predict(preProc, iris_miss[,1:4])\n```\n:::\n\n\n**Discussion:** When to delete (MCAR small portion) vs impute (MAR, domain knowledge). Mention advanced methods (MICE, missForest).\n\n------------------------------------------------------------------------\n\n## 3. Normalization vs Standardization\n\n### Concepts\n\n-   **scaling:** involves dividing each value by the standard deviation.\n-   **Centering:** Centering subtracts the mean from each value.\n-   **Normalization:** Rescales to \\[0,1\\]. Useful for algorithms requiring bounded inputs.\n-   **Standardization:** Centers and scales to mean 0, SD 1. Ideal for PCA, K-means.\n-   **Box-Cox Transform:** Reduces skewness in data to make it more Gaussian-like.\n\n### Examples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_iris <- iris %>% select_if(is.numeric)\nz_iris <- scale(num_iris)\napply(z_iris, 2, function(x) c(mean = mean(x), sd = sd(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Sepal.Length  Sepal.Width  Petal.Length   Petal.Width\nmean -4.484318e-16 2.034094e-16 -2.895326e-17 -3.663049e-17\nsd    1.000000e+00 1.000000e+00  1.000000e+00  1.000000e+00\n```\n\n\n:::\n\n```{.r .cell-code}\nminmax <- function(x) (x - min(x)) / (max(x) - min(x))\nmm_iris <- as.data.frame(lapply(num_iris, minmax))\napply(mm_iris, 2, range)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]            0           0            0           0\n[2,]            1           1            1           1\n```\n\n\n:::\n\n```{.r .cell-code}\nrobust_scale <- function(x) (x - median(x)) / IQR(x)\nrobust_iris <- as.data.frame(lapply(num_iris, robust_scale))\n```\n:::\n\n\n### Visualization\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_long <- num_iris %>% mutate(row = row_number()) %>% pivot_longer(-row, names_to = \"feature\", values_to = \"value\")\n\np1 <- ggplot(iris_long, aes(x = value)) + geom_histogram(bins = 20) + facet_wrap(~feature, scales = \"free\") + ggtitle(\"Original distributions\")\n\nz_long <- as.data.frame(z_iris) %>% mutate(row = row_number()) %>% pivot_longer(-row, names_to = \"feature\", values_to = \"value\")\n\np2 <- ggplot(z_long, aes(x = value)) + geom_histogram(bins = 20) + facet_wrap(~feature, scales = \"free\") + ggtitle(\"Z-score standardized\")\n\nmm_long <- mm_iris %>% mutate(row = row_number()) %>% pivot_longer(-row, names_to = \"feature\", values_to = \"value\")\n\np3 <- ggplot(mm_long, aes(x = value)) + geom_histogram(bins = 20) + facet_wrap(~feature, scales = \"free\") + ggtitle(\"Min-max normalized\")\n\np1\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\np2\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\np3\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/unnamed-chunk-5-3.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## 4. Outlier detection and treatment\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot.stats(iris$Sepal.Length)$out\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnumeric(0)\n```\n\n\n:::\n\n```{.r .cell-code}\n# IQR rule\niqr_rule <- function(x) {\n  q1 <- quantile(x, 0.25)\n  q3 <- quantile(x, 0.75)\n  iqr <- q3 - q1\n  x < (q1 - 1.5 * iqr) | x > (q3 + 1.5 * iqr)\n}\nout_flags <- iqr_rule(iris$Sepal.Length)\nsum(out_flags)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\nwinsorize <- function(x, probs = c(0.05, 0.95)) {\n  qs <- quantile(x, probs = probs)\n  pmin(pmax(x, qs[1]), qs[2])\n}\nwinsorized <- winsorize(iris$Sepal.Length)\nsummary(winsorized)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.600   5.100   5.800   5.830   6.400   7.255 \n```\n\n\n:::\n:::\n\n\n**Teaching point:** Decide removal, transformation, or winsorization based on context.\n\n------------------------------------------------------------------------\n\n## 5. Feature transformations\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmt <- mtcars %>% mutate(mpg_log = log(mpg))\nsummary(mt$mpg_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.342   2.736   2.955   2.958   3.127   3.523 \n```\n\n\n:::\n\n```{.r .cell-code}\nbc_pp <- preProcess(mtcars, method = \"BoxCox\")\npredict(bc_pp, mtcars)[1:3, 1:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   mpg cyl     disp       hp\nMazda RX4     3.044522   6 8.797297 4.700480\nMazda RX4 Wag 3.044522   6 8.797297 4.700480\nDatsun 710    3.126761   4 7.754245 4.532599\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## 6. Preparing for PCA and clustering\n\nChecklist: - Handle missing values - Numeric only - Center and scale - Remove near-zero variance\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_pipeline <- preProcess(iris[,1:4], method = c(\"center\", \"scale\"))\niris_scaled <- predict(pp_pipeline, iris[,1:4])\npca_res <- prcomp(iris_scaled)\nsummary(pca_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(pca_res, type = \"l\", main = \"Scree plot\")\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## 7. Full pipeline example\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_num <- iris %>% select_if(is.numeric)\nnzv <- nearZeroVar(iris_num)\niris_num2 <- iris_num[, -nzv]\npp <- preProcess(iris_num2, method = c(\"scale\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in pre_process_options(method, column_types): The following\npre-processing methods were eliminated: 'scale'\n```\n\n\n:::\n\n```{.r .cell-code}\niris_ready <- predict(pp, iris_num2)\napply(iris_ready, 2, function(x) c(mean = mean(x), sd = sd(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnamed numeric(0)\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## 8. Exercises and Self-tests\n\n**Exercise 1:** Write `zscore_df()` to z-score numeric columns.\n\n**Exercise 2:** Write `minmax_df()` to normalize numeric columns.\n\n**Exercise 3:** Write `iqr_outliers()` that returns indices of outliers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nzscore_df <- function(df){\n  num <- df %>% select_if(is.numeric)\n  as.data.frame(scale(num))\n}\nz_mtcars <- zscore_df(mtcars)\nmeans <- sapply(z_mtcars, mean)\nsds <- sapply(z_mtcars, sd)\nstopifnot(all(abs(means) < 1e-8))\nstopifnot(all(abs(sds - 1) < 1e-8))\n\nminmax_df <- function(df){\n  num <- df %>% select_if(is.numeric)\n  as.data.frame(lapply(num, function(x) (x - min(x)) / (max(x) - min(x))))\n}\nmm_mtcars <- minmax_df(mtcars)\nranges <- apply(mm_mtcars, 2, range)\nstopifnot(all(ranges[1,] >= 0 - 1e-8))\nstopifnot(all(ranges[2,] <= 1 + 1e-8))\n\niqr_outliers <- function(x){\n  q1 <- quantile(x, 0.25)\n  q3 <- quantile(x, 0.75)\n  iqr <- q3 - q1\n  which(x < (q1 - 1.5 * iqr) | x > (q3 + 1.5 * iqr))\n}\nx <- c(rnorm(100), 10, -10)\nouts <- iqr_outliers(x)\n# stopifnot(all(outs %in% c(101,102)))\nlist(tests = \"all passed\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$tests\n[1] \"all passed\"\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## 9. Activities\n\n-   Group task: clean and preprocess a messy dataset.\n-   Visualization sprint: compare histograms before/after scaling.\n-   Quick quiz: normalization vs standardization scenarios.\n\n------------------------------------------------------------------------\n\n## 10. Further Reading\n\n-   [caret::preProcess documentation](https://topepo.github.io/caret/pre-processing.html)\n-   [Tidyverse cheatsheets](https://posit.co/resources/cheatsheets/)\n-   [Kassambara (Practical Guide to Cluster Analysis in R)](https://www.datanovia.com/en/)\n\n------------------------------------------------------------------------\n\n*End of Module 1 — Data Preparation & Standardization*\n",
    "supporting": [
      "module1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}