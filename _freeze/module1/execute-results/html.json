{
  "hash": "782cfab80299f00b79d912ef9a050b37",
  "result": {
    "engine": "knitr",
    "markdown": "# Module 1 — Data Preparation & Standardization\n\n## Learning Outcomes\n\nBy the end of this module learners will be able to:\n\n-   Explain the difference between normalization and standardization and when to use each.\n-   Apply scaling methods in R (`scale()`, `caret::preProcess`) and create min-max and robust scaling.\n-   Detect and treat missing values and common outliers.\n-   Prepare data for PCA and clustering (center, scale, and check assumptions).\n-   Write small self-check tests to verify preprocessing steps.\n\n------------------------------------------------------------------------\n\n## 1. Why preprocessing matters\n\nMany algorithms (like K-means, PCA, distance-based methods) assume features are on comparable scales. Preprocessing ensures that units and magnitudes don’t distort the analysis.\n\n\n::: {.cell}\n\n:::\n\n\n------------------------------------------------------------------------\n\n## 2. Data types and missing values\n\n### Inspecting structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n```\n\n\n:::\n:::\n\n\n### Handling missing values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\niris_miss <- iris \nidx <- sample(seq_len(nrow(iris_miss)), size = floor(0.05 * nrow(iris_miss) ))\niris_miss$Sepal.Length[idx] <- NA\nsum(is.na(iris_miss$Sepal.Length))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7\n```\n\n\n:::\n\n```{.r .cell-code}\niris_drop <- na.omit(iris_miss)\niris_meanimp <- iris_miss\niris_meanimp$Sepal.Length[is.na(iris_meanimp$Sepal.Length)] <- mean(iris_meanimp$Sepal.Length, na.rm = TRUE)\n\n#iris_miss %>% mutate(Sepal.Length = tidyr::replace_na(Sepal.Length,mean(iris_miss$Sepal.Length,na.rm = T))) %>% head()\n\n# preProc <- preProcess(iris_miss[,1:4], method = c(\"knnImpute\"))\n# iris_knnimp <- predict(preProc, iris_miss[,1:4])\n```\n:::\n\n\n**Discussion:** When to delete (MCAR small portion) vs impute (MAR, domain knowledge). Mention advanced methods (MICE, missForest).\n\n------------------------------------------------------------------------\n\n## 3. Normalization vs Standardization\n\n### Concepts\n\n-   **scaling:** involves dividing each value by the standard deviation.\n-   **Centering:** Centering subtracts the mean from each value.\n-   **Normalization:** Rescales to \\[0,1\\]. Useful for algorithms requiring bounded inputs.\n-   **Standardization:** Centers and scales to mean 0, SD 1. Ideal for PCA, K-means.\n-   **Box-Cox Transform:** Reduces skewness in data to make it more Gaussian-like.\n\n### Examples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_iris <- iris %>% select_if(is.numeric)\nz_iris <- scale(num_iris)\napply(z_iris, 2, function(x) c(mean = mean(x), sd = sd(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Sepal.Length  Sepal.Width  Petal.Length   Petal.Width\nmean -4.484318e-16 2.034094e-16 -2.895326e-17 -3.663049e-17\nsd    1.000000e+00 1.000000e+00  1.000000e+00  1.000000e+00\n```\n\n\n:::\n\n```{.r .cell-code}\nminmax <- function(x) (x - min(x)) / (max(x) - min(x))\nmm_iris <- as.data.frame(lapply(num_iris, minmax))\napply(mm_iris, 2, range)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]            0           0            0           0\n[2,]            1           1            1           1\n```\n\n\n:::\n\n```{.r .cell-code}\nrobust_scale <- function(x) (x - median(x)) / IQR(x)\nrobust_iris <- as.data.frame(lapply(num_iris, robust_scale))\n```\n:::\n\n\n### Visualization\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_long <- num_iris %>% mutate(row = row_number()) %>% pivot_longer(-row, names_to = \"feature\", values_to = \"value\")\n\np1 <- ggplot(iris_long, aes(x = value)) + geom_histogram(bins = 20) + facet_wrap(~feature, scales = \"free\") + ggtitle(\"Original distributions\")\n\nz_long <- as.data.frame(z_iris) %>% mutate(row = row_number()) %>% pivot_longer(-row, names_to = \"feature\", values_to = \"value\")\n\np2 <- ggplot(z_long, aes(x = value)) + geom_histogram(bins = 20) + facet_wrap(~feature, scales = \"free\") + ggtitle(\"Z-score standardized\")\n\nmm_long <- mm_iris %>% mutate(row = row_number()) %>% pivot_longer(-row, names_to = \"feature\", values_to = \"value\")\n\np3 <- ggplot(mm_long, aes(x = value)) + geom_histogram(bins = 20) + facet_wrap(~feature, scales = \"free\") + ggtitle(\"Min-max normalized\")\n\np1\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\np2\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\np3\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/unnamed-chunk-5-3.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## 4. Outlier detection and treatment\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot.stats(iris$Sepal.Length)$out\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnumeric(0)\n```\n\n\n:::\n\n```{.r .cell-code}\n# IQR rule\niqr_rule <- function(x) {\n  q1 <- quantile(x, 0.25)\n  q3 <- quantile(x, 0.75)\n  iqr <- q3 - q1\n  x < (q1 - 1.5 * iqr) |  x > (q3 + 1.5 * iqr)\n}\nout_flags <- iqr_rule(iris$Sepal.Length)\nsum(out_flags)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\nwinsorize <- function(x, probs = c(0.05, 0.95)) {\n  qs <- quantile(x, probs = probs)\n  pmin(pmax(x, qs[1]), qs[2])\n}\nwinsorized <- winsorize(iris$Sepal.Length)\nsummary(winsorized)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.600   5.100   5.800   5.830   6.400   7.255 \n```\n\n\n:::\n:::\n\n\n**Teaching point:** Decide removal, transformation, or winsorization based on context.\n\n------------------------------------------------------------------------\n\n## 5. Feature transformations\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmt <- mtcars %>% mutate(mpg_log = log(mpg))\nsummary(mt$mpg_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.342   2.736   2.955   2.958   3.127   3.523 \n```\n\n\n:::\n\n```{.r .cell-code}\nbc_pp <- preProcess(mtcars, method = \"BoxCox\")\npredict(bc_pp, mtcars)[1:3, 1:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   mpg cyl     disp       hp\nMazda RX4     3.044522   6 8.797297 4.700480\nMazda RX4 Wag 3.044522   6 8.797297 4.700480\nDatsun 710    3.126761   4 7.754245 4.532599\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## 6. Principle Component Analysis\n\nPrinciple Component Analysis is a transformation technique that focuses on dimensionaliity reduction. This entails transforming a dataset with a large number of variables into less variables that contain most of the information of the affected variables. The technique is split into 3 concepts as below\n\n-   Dimensionality Reduction where the large number of variables are condensed into smaller variables that are a representation of the large ones.\n-   Principal Components which are the new variables. These are a linear combination of the original variables and are ordered by the amount of the variance they capture.\n-   Variance: PCA assumes that the information is carried in the variance of the features. The higher the variation in a feature, the more information it carries.\n\n\n### Preparing for PCA\n\nChecklist: - Handle missing values - Numeric only - Center and scale / standardization - Remove near-zero variance\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_pipeline <- preProcess(iris[,1:4], method = c(\"center\", \"scale\")) ## centering and scaling the data set\niris_scaled <- predict(pp_pipeline, iris[,1:4]) ## applying the behavior to the data set\npca_res <- prcomp(iris_scaled) ## getting the principle components\n# summary(pca_res)\n# plot(pca_res, type = \"l\", main = \"Scree plot\")\n\npca_dta <- pca_res$x %>%data.frame() \npca_dta%>% head() %>%   flextable::flextable() %>% flextable::autofit()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-03019026{}.cl-02cbd774{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-02de08c2{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-02de6cae{width:0.973in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-02de6ccc{width:1.058in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-02de6ce0{width:1.143in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-02de6cea{width:1.228in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-02de6cf4{width:0.973in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-02de6cfe{width:1.058in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-02de6d08{width:1.143in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-02de6d09{width:1.228in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-02de6d12{width:0.973in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-02de6d1c{width:1.058in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-02de6d26{width:1.143in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-02de6d27{width:1.228in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-03019026'><thead><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-02de6cae\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">PC1</span></p></th><th class=\"cl-02de6ccc\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">PC2</span></p></th><th class=\"cl-02de6ce0\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">PC3</span></p></th><th class=\"cl-02de6cea\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">PC4</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-02de6cf4\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">-2.257141</span></p></td><td class=\"cl-02de6cfe\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">-0.4784238</span></p></td><td class=\"cl-02de6d08\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">0.12727962</span></p></td><td class=\"cl-02de6d09\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">0.024087508</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-02de6cf4\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">-2.074013</span></p></td><td class=\"cl-02de6cfe\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">0.6718827</span></p></td><td class=\"cl-02de6d08\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">0.23382552</span></p></td><td class=\"cl-02de6d09\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">0.102662845</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-02de6cf4\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">-2.356335</span></p></td><td class=\"cl-02de6cfe\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">0.3407664</span></p></td><td class=\"cl-02de6d08\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">-0.04405390</span></p></td><td class=\"cl-02de6d09\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">0.028282305</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-02de6cf4\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">-2.291707</span></p></td><td class=\"cl-02de6cfe\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">0.5953999</span></p></td><td class=\"cl-02de6d08\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">-0.09098530</span></p></td><td class=\"cl-02de6d09\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">-0.065735340</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-02de6cf4\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">-2.381863</span></p></td><td class=\"cl-02de6cfe\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">-0.6446757</span></p></td><td class=\"cl-02de6d08\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">-0.01568565</span></p></td><td class=\"cl-02de6d09\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">-0.035802870</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-02de6d12\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">-2.068701</span></p></td><td class=\"cl-02de6d1c\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">-1.4842053</span></p></td><td class=\"cl-02de6d26\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">-0.02687825</span></p></td><td class=\"cl-02de6d27\"><p class=\"cl-02de08c2\"><span class=\"cl-02cbd774\">0.006586116</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\nVisualizing the scree plot. This is used to visualize the eigenvalues or the proportion of variance explained by each principal component (PC).\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(factoextra)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'factoextra' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(FactoMineR)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'FactoMineR' was built under R version 4.3.3\n```\n\n\n:::\n\n```{.r .cell-code}\nfviz_eig(pca_res, \n         addlabels = TRUE, \n         ylim = c(0, 70),\n         main=\"Figure 5: Scree Plot\")\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nExplaining the variance\n\n::: {.cell}\n\n```{.r .cell-code}\npca_var <- pca_res$sdev^2\npca_var / sum(pca_var)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.729624454 0.228507618 0.036689219 0.005178709\n```\n\n\n:::\n:::\n\n\nComparing the PCA output and the current classifications\n\n::: {.cell}\n\n```{.r .cell-code}\niris_pca <- pca_dta %>% bind_cols(iris %>% select(Species))\n\npca_dta$Species = iris$Species\n\n# iris_pca %>% \n  ggplot(iris_pca,\n       aes(x = PC1, \n       y = PC2, \n       color = Species)) +\n       geom_point() +\n       scale_color_manual(values=c(\"black\", \"#CC0066\", \"green2\")) +\n       stat_ellipse() + ggtitle(\"Ellipse Plot\") +\n       theme_bw()+\n  theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## 7. Clustering \n\n### Learning Outcomes \nBy the end of this module, learners will be able to:\n-   Prepare data for clustering (scaling, distance measures, PCA-based preprocessing).\n-   Perform K-Means, Hierarchical, and Density-Based clustering.\n-   Evaluate clusters using Silhouette Width, Dunn Index, and internal metrics.\n-   Interpret clusters visually using PCA and advanced plotting.\n\n### 1. Data Preparation for Clustering \n  \nBefore clustering, ensure variables are scaled correctly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscaled_data <- scale(iris[,1:4])\nhead(scaled_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]   -0.8976739  1.01560199    -1.335752   -1.311052\n[2,]   -1.1392005 -0.13153881    -1.335752   -1.311052\n[3,]   -1.3807271  0.32731751    -1.392399   -1.311052\n[4,]   -1.5014904  0.09788935    -1.279104   -1.311052\n[5,]   -1.0184372  1.24503015    -1.335752   -1.311052\n[6,]   -0.5353840  1.93331463    -1.165809   -1.048667\n```\n\n\n:::\n:::\n\n\n**Key concepts:**\n-   Distance-based algorithms require standardized input.\n-   PCA helps reduce multicollinearity and compress correlated features.\n\n---\n  \n### 2. K-Means Clustering (Deep Dive)\n  \n#### 2.1 Running K-Means\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nkmeans_fit <- kmeans(scaled_data, centers = 3, nstart = 25)\nkmeans_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nK-means clustering with 3 clusters of sizes 50, 53, 47\n\nCluster means:\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1  -1.01119138  0.85041372   -1.3006301  -1.2507035\n2  -0.05005221 -0.88042696    0.3465767   0.2805873\n3   1.13217737  0.08812645    0.9928284   1.0141287\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 2 2 2 3 2 2 2 2 2 2 2 2 3 2 2 2 2 3 2 2 2\n [75] 2 3 3 3 2 2 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3\n[112] 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 3 3 3 3 3 3 2 2 3 3 3 2 3 3 3 2 3 3 3 2 3\n[149] 3 2\n\nWithin cluster sum of squares by cluster:\n[1] 47.35062 44.08754 47.45019\n (between_SS / total_SS =  76.7 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n```\n\n\n:::\n:::\n\n\n### 2.2 Visualizing K-Means Clusters\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Scale data\nscaled_data <- scale(iris[,1:4])\n\n# 2. Run PCA\npca1 <- prcomp(scaled_data)\npca_scores <- as.data.frame(pca1$x[, 1:2])  # PC1 and PC2\n\n# 3. Run K-means \nset.seed(123)\nkmeans_fit <- kmeans(scaled_data, centers = 3, nstart = 25)\n\n# 4. Build dataframe for plotting\ncluster_df <- cbind(pca_scores, cluster = as.factor(kmeans_fit$cluster))\n\n# 5. Plot\nggplot(cluster_df, aes(PC1, PC2, color = cluster)) +\n  geom_point(size = 3) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n#### 2.3 Cluster Evaluation\n  \n**Elbow Method:**\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nels <- vector()\nfor (k in 1:10) {\n  els[k] <- kmeans(scaled_data, centers = k, nstart = 20)$tot.withinss\n}\nplot(1:10, els, type = \"b\", pch = 19)\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nusing another approach that considers the average silhouette \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_nbclust(scaled_data, kmeans, method = \"silhouette\")\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n**Silhouette Score:**\n  \nThe Silhouette Score is used to evaluate the quality of clusters in a clustering algorithm. It measures how similar a sample is to its own cluster compared to other clusters. The score ranges from -1 to 1, where a higher score indicates better-defined clusters.A score close to 1 indicates that the sample is well-clustered, a score close to 0 indicates overlapping clusters, and a negative score indicates that the sample might be assigned to the wrong cluster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cluster)\nsil <- silhouette(kmeans_fit$cluster, dist(scaled_data))\nplot(sil)\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmean(sil[,3])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4599482\n```\n\n\n:::\n:::\n\n\nInterpretation: Scores close to **1** indicate well-separated clusters.\n\n### 3. Hierarchical Clustering\n  \n\n::: {.cell}\n\n```{.r .cell-code}\ndist_mat <- dist(scaled_data)\nfit_hc <- hclust(dist_mat, method = \"ward.D2\")\nplot(fit_hc)\nrect.hclust(fit_hc, k = 3, border = \"red\")\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n**Discussion:** When to use hierarchical vs K-means.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### 4. DBSCAN (Density-Based Clustering)\n  \n# library(dbscan)\n# set.seed(42)\n# db <- dbscan(scaled_data, eps = 0.5, minPts = 5)\n# plot(db, scaled_data)\n```\n:::\n\n\nUseful for non-spherical clusters; can detect noise.\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## 7. Full pipeline example\n\n# iris_num <- iris %>% select_if(is.numeric)\n# nzv <- nearZeroVar(iris_num)\n# iris_num2 <- iris_num[, -nzv]\n# pp <- preProcess(iris_num2, method = c(\"scale\"))\n# iris_ready <- predict(pp, iris_num2)\n# apply(iris_ready, 2, function(x) c(mean = mean(x), sd = sd(x)))\n```\n:::\n\n\n------------------------------------------------------------------------\n\n## 8. Exercises and Self-tests\n\n**Exercise 1:** Write `zscore_df()` to z-score numeric columns.\n\n**Exercise 2:** Write `minmax_df()` to normalize numeric columns.\n\n**Exercise 3:** Write `iqr_outliers()` that returns indices of outliers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nzscore_df <- function(df){\n  num <- df %>% select_if(is.numeric)\n  as.data.frame(scale(num))\n}\nz_mtcars <- zscore_df(mtcars)\nmeans <- sapply(z_mtcars, mean)\nsds <- sapply(z_mtcars, sd)\nstopifnot(all(abs(means) < 1e-8))\nstopifnot(all(abs(sds - 1) < 1e-8))\n\nminmax_df <- function(df){\n  num <- df %>% select_if(is.numeric)\n  as.data.frame(lapply(num, function(x) (x - min(x)) / (max(x) - min(x))))\n}\nmm_mtcars <- minmax_df(mtcars)\nranges <- apply(mm_mtcars, 2, range)\nstopifnot(all(ranges[1,] >= 0 - 1e-8))\nstopifnot(all(ranges[2,] <= 1 + 1e-8))\n\niqr_outliers <- function(x){\n  q1 <- quantile(x, 0.25)\n  q3 <- quantile(x, 0.75)\n  iqr <- q3 - q1\n  which(x < (q1 - 1.5 * iqr) | x > (q3 + 1.5 * iqr))\n}\nx <- c(rnorm(100), 10, -10)\nouts <- iqr_outliers(x)\n# stopifnot(all(outs %in% c(101,102)))\nlist(tests = \"all passed\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$tests\n[1] \"all passed\"\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## 9. Activities\n\n-   Group task: clean and preprocess a messy dataset.\n-   Visualization sprint: compare histograms before/after scaling.\n-   Quick quiz: normalization vs standardization scenarios.\n-   PCA: Perform PCA on the wine dataset and interpret first three PCs\n\n------------------------------------------------------------------------\n\n## 10. Further Reading\n\n-   [caret::preProcess documentation](https://topepo.github.io/caret/pre-processing.html)\n-   [Tidyverse cheatsheets](https://posit.co/resources/cheatsheets/)\n-   [Kassambara (Practical Guide to Cluster Analysis in R)](https://www.datanovia.com/en/)\n\n------------------------------------------------------------------------\n\n*End of Module 1 — Data Preparation & Standardization*\n",
    "supporting": [
      "module1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/tabwid-1.1.3/tabwid.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/tabwid-1.1.3/tabwid.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}