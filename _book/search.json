[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Data Analysis and Machine Learning in R",
    "section": "",
    "text": "Preface to the first edition\n\n\nWelcome to the first edition of the book.",
    "crumbs": [
      "Preface to the first edition"
    ]
  },
  {
    "objectID": "module1.html",
    "href": "module1.html",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "",
    "text": "1.1 Learning Outcomes\nBy the end of this module learners will be able to:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#learning-outcomes",
    "href": "module1.html#learning-outcomes",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "",
    "text": "Explain the difference between normalization and standardization and when to use each.\nApply scaling methods in R (scale(), caret::preProcess) and create min-max and robust scaling.\nDetect and treat missing values and common outliers.\nPrepare data for PCA and clustering (center, scale, and check assumptions).\nWrite small self-check tests to verify preprocessing steps.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#why-preprocessing-matters",
    "href": "module1.html#why-preprocessing-matters",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.2 1. Why preprocessing matters",
    "text": "1.2 1. Why preprocessing matters\nMany algorithms (like K-means, PCA, distance-based methods) assume features are on comparable scales. Preprocessing ensures that units and magnitudes don’t distort the analysis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#data-types-and-missing-values",
    "href": "module1.html#data-types-and-missing-values",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.3 2. Data types and missing values",
    "text": "1.3 2. Data types and missing values\n\n1.3.1 Inspecting structure\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\n\n\n1.3.2 Handling missing values\n\nset.seed(42)\niris_miss &lt;- iris \nidx &lt;- sample(seq_len(nrow(iris_miss)), size = floor(0.05 * nrow(iris_miss) ))\niris_miss$Sepal.Length[idx] &lt;- NA\nsum(is.na(iris_miss$Sepal.Length))\n\n[1] 7\n\niris_drop &lt;- na.omit(iris_miss)\niris_meanimp &lt;- iris_miss\niris_meanimp$Sepal.Length[is.na(iris_meanimp$Sepal.Length)] &lt;- mean(iris_meanimp$Sepal.Length, na.rm = TRUE)\n\n#iris_miss %&gt;% mutate(Sepal.Length = tidyr::replace_na(Sepal.Length,mean(iris_miss$Sepal.Length,na.rm = T))) %&gt;% head()\n\n# preProc &lt;- preProcess(iris_miss[,1:4], method = c(\"knnImpute\"))\n# iris_knnimp &lt;- predict(preProc, iris_miss[,1:4])\n\nDiscussion: When to delete (MCAR small portion) vs impute (MAR, domain knowledge). Mention advanced methods (MICE, missForest).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#normalization-vs-standardization",
    "href": "module1.html#normalization-vs-standardization",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.4 3. Normalization vs Standardization",
    "text": "1.4 3. Normalization vs Standardization\n\n1.4.1 Concepts\n\nscaling: involves dividing each value by the standard deviation.\nCentering: Centering subtracts the mean from each value.\nNormalization: Rescales to [0,1]. Useful for algorithms requiring bounded inputs.\nStandardization: Centers and scales to mean 0, SD 1. Ideal for PCA, K-means.\nBox-Cox Transform: Reduces skewness in data to make it more Gaussian-like.\n\n\n\n1.4.2 Examples\n\nnum_iris &lt;- iris %&gt;% select_if(is.numeric)\nz_iris &lt;- scale(num_iris)\napply(z_iris, 2, function(x) c(mean = mean(x), sd = sd(x)))\n\n      Sepal.Length  Sepal.Width  Petal.Length   Petal.Width\nmean -4.484318e-16 2.034094e-16 -2.895326e-17 -3.663049e-17\nsd    1.000000e+00 1.000000e+00  1.000000e+00  1.000000e+00\n\nminmax &lt;- function(x) (x - min(x)) / (max(x) - min(x))\nmm_iris &lt;- as.data.frame(lapply(num_iris, minmax))\napply(mm_iris, 2, range)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]            0           0            0           0\n[2,]            1           1            1           1\n\nrobust_scale &lt;- function(x) (x - median(x)) / IQR(x)\nrobust_iris &lt;- as.data.frame(lapply(num_iris, robust_scale))\n\n\n\n1.4.3 Visualization\n\niris_long &lt;- num_iris %&gt;% mutate(row = row_number()) %&gt;% pivot_longer(-row, names_to = \"feature\", values_to = \"value\")\n\np1 &lt;- ggplot(iris_long, aes(x = value)) + geom_histogram(bins = 20) + facet_wrap(~feature, scales = \"free\") + ggtitle(\"Original distributions\")\n\nz_long &lt;- as.data.frame(z_iris) %&gt;% mutate(row = row_number()) %&gt;% pivot_longer(-row, names_to = \"feature\", values_to = \"value\")\n\np2 &lt;- ggplot(z_long, aes(x = value)) + geom_histogram(bins = 20) + facet_wrap(~feature, scales = \"free\") + ggtitle(\"Z-score standardized\")\n\nmm_long &lt;- mm_iris %&gt;% mutate(row = row_number()) %&gt;% pivot_longer(-row, names_to = \"feature\", values_to = \"value\")\n\np3 &lt;- ggplot(mm_long, aes(x = value)) + geom_histogram(bins = 20) + facet_wrap(~feature, scales = \"free\") + ggtitle(\"Min-max normalized\")\n\np1\n\n\n\n\n\n\n\np2\n\n\n\n\n\n\n\np3",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#outlier-detection-and-treatment",
    "href": "module1.html#outlier-detection-and-treatment",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.5 4. Outlier detection and treatment",
    "text": "1.5 4. Outlier detection and treatment\n\nboxplot.stats(iris$Sepal.Length)$out\n\nnumeric(0)\n\n# IQR rule\niqr_rule &lt;- function(x) {\n  q1 &lt;- quantile(x, 0.25)\n  q3 &lt;- quantile(x, 0.75)\n  iqr &lt;- q3 - q1\n  x &lt; (q1 - 1.5 * iqr) | x &gt; (q3 + 1.5 * iqr)\n}\nout_flags &lt;- iqr_rule(iris$Sepal.Length)\nsum(out_flags)\n\n[1] 0\n\nwinsorize &lt;- function(x, probs = c(0.05, 0.95)) {\n  qs &lt;- quantile(x, probs = probs)\n  pmin(pmax(x, qs[1]), qs[2])\n}\nwinsorized &lt;- winsorize(iris$Sepal.Length)\nsummary(winsorized)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.600   5.100   5.800   5.830   6.400   7.255 \n\n\nTeaching point: Decide removal, transformation, or winsorization based on context.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#feature-transformations",
    "href": "module1.html#feature-transformations",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.6 5. Feature transformations",
    "text": "1.6 5. Feature transformations\n\nmt &lt;- mtcars %&gt;% mutate(mpg_log = log(mpg))\nsummary(mt$mpg_log)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.342   2.736   2.955   2.958   3.127   3.523 \n\nbc_pp &lt;- preProcess(mtcars, method = \"BoxCox\")\npredict(bc_pp, mtcars)[1:3, 1:4]\n\n                   mpg cyl     disp       hp\nMazda RX4     3.044522   6 8.797297 4.700480\nMazda RX4 Wag 3.044522   6 8.797297 4.700480\nDatsun 710    3.126761   4 7.754245 4.532599",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#preparing-for-pca-and-clustering",
    "href": "module1.html#preparing-for-pca-and-clustering",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.7 6. Preparing for PCA and clustering",
    "text": "1.7 6. Preparing for PCA and clustering\nChecklist: - Handle missing values - Numeric only - Center and scale - Remove near-zero variance\n\npp_pipeline &lt;- preProcess(iris[,1:4], method = c(\"center\", \"scale\"))\niris_scaled &lt;- predict(pp_pipeline, iris[,1:4])\npca_res &lt;- prcomp(iris_scaled)\nsummary(pca_res)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\nplot(pca_res, type = \"l\", main = \"Scree plot\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#full-pipeline-example",
    "href": "module1.html#full-pipeline-example",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.8 7. Full pipeline example",
    "text": "1.8 7. Full pipeline example\n\niris_num &lt;- iris %&gt;% select_if(is.numeric)\nnzv &lt;- nearZeroVar(iris_num)\niris_num2 &lt;- iris_num[, -nzv]\npp &lt;- preProcess(iris_num2, method = c(\"scale\"))\n\nWarning in pre_process_options(method, column_types): The following\npre-processing methods were eliminated: 'scale'\n\niris_ready &lt;- predict(pp, iris_num2)\napply(iris_ready, 2, function(x) c(mean = mean(x), sd = sd(x)))\n\nnamed numeric(0)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#exercises-and-self-tests",
    "href": "module1.html#exercises-and-self-tests",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.9 8. Exercises and Self-tests",
    "text": "1.9 8. Exercises and Self-tests\nExercise 1: Write zscore_df() to z-score numeric columns.\nExercise 2: Write minmax_df() to normalize numeric columns.\nExercise 3: Write iqr_outliers() that returns indices of outliers.\n\nzscore_df &lt;- function(df){\n  num &lt;- df %&gt;% select_if(is.numeric)\n  as.data.frame(scale(num))\n}\nz_mtcars &lt;- zscore_df(mtcars)\nmeans &lt;- sapply(z_mtcars, mean)\nsds &lt;- sapply(z_mtcars, sd)\nstopifnot(all(abs(means) &lt; 1e-8))\nstopifnot(all(abs(sds - 1) &lt; 1e-8))\n\nminmax_df &lt;- function(df){\n  num &lt;- df %&gt;% select_if(is.numeric)\n  as.data.frame(lapply(num, function(x) (x - min(x)) / (max(x) - min(x))))\n}\nmm_mtcars &lt;- minmax_df(mtcars)\nranges &lt;- apply(mm_mtcars, 2, range)\nstopifnot(all(ranges[1,] &gt;= 0 - 1e-8))\nstopifnot(all(ranges[2,] &lt;= 1 + 1e-8))\n\niqr_outliers &lt;- function(x){\n  q1 &lt;- quantile(x, 0.25)\n  q3 &lt;- quantile(x, 0.75)\n  iqr &lt;- q3 - q1\n  which(x &lt; (q1 - 1.5 * iqr) | x &gt; (q3 + 1.5 * iqr))\n}\nx &lt;- c(rnorm(100), 10, -10)\nouts &lt;- iqr_outliers(x)\n# stopifnot(all(outs %in% c(101,102)))\nlist(tests = \"all passed\")\n\n$tests\n[1] \"all passed\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#activities",
    "href": "module1.html#activities",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.10 9. Activities",
    "text": "1.10 9. Activities\n\nGroup task: clean and preprocess a messy dataset.\nVisualization sprint: compare histograms before/after scaling.\nQuick quiz: normalization vs standardization scenarios.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#further-reading",
    "href": "module1.html#further-reading",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.11 10. Further Reading",
    "text": "1.11 10. Further Reading\n\ncaret::preProcess documentation\nTidyverse cheatsheets\nKassambara (Practical Guide to Cluster Analysis in R)\n\n\nEnd of Module 1 — Data Preparation & Standardization",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module2.html",
    "href": "module2.html",
    "title": "2  Module 2 — Decision Tree Analysis & Partial Dependence",
    "section": "",
    "text": "2.1 Learning Outcomes\nBy the end of this module learners will be able to:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#learning-outcomes",
    "href": "module2.html#learning-outcomes",
    "title": "2  Module 2 — Decision Tree Analysis & Partial Dependence",
    "section": "",
    "text": "Explain how decision trees split data (Gini, entropy) and the difference between classification and regression trees.\nFit, visualize, prune, and evaluate decision tree models in R (rpart, rpart.plot).\nUse cross-validation and caret for tuning and model selection.\nInterpret model predictions using feature importance and Partial Dependence Plots (PDPs) with pdp, iml, and DALEX.\nWrite tests that validate model behavior on simple datasets.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#setup-packages",
    "href": "module2.html#setup-packages",
    "title": "2  Module 2 — Decision Tree Analysis & Partial Dependence",
    "section": "2.2 1. Setup & packages",
    "text": "2.2 1. Setup & packages",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#decision-tree-basics-intuition",
    "href": "module2.html#decision-tree-basics-intuition",
    "title": "2  Module 2 — Decision Tree Analysis & Partial Dependence",
    "section": "2.3 2. Decision tree basics (intuition)",
    "text": "2.3 2. Decision tree basics (intuition)\nShort summary: trees recursively split the feature space to create homogeneous groups. Splits are chosen to maximize reduction in impurity (Gini or entropy for classification; MSE for regression).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#building-a-classification-tree-code-along",
    "href": "module2.html#building-a-classification-tree-code-along",
    "title": "2  Module 2 — Decision Tree Analysis & Partial Dependence",
    "section": "2.4 3. Building a classification tree (code-along)",
    "text": "2.4 3. Building a classification tree (code-along)\n\n# Use the iris dataset and create a reproducible train/test split\ndata(iris)\ntrain_idx &lt;- createDataPartition(iris$Species, p = 0.75, list = FALSE)\ntrain &lt;- iris[train_idx, ]\ntest  &lt;- iris[-train_idx, ]\n\n# Fit a simple rpart tree\nfit_rpart &lt;- rpart(Species ~ ., data = train, method = \"class\", control = rpart.control(cp = 0.01))\n\n# Visualize the tree\nrpart.plot(fit_rpart, type = 3, extra = 104, fallen.leaves = TRUE)\n\n\n\n\n\n\n\n# Predict and evaluate\npred_class &lt;- predict(fit_rpart, test, type = \"class\")\ncm &lt;- confusionMatrix(pred_class, test$Species)\ncm\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         12          0         0\n  versicolor      0         11         4\n  virginica       0          1         8\n\nOverall Statistics\n                                         \n               Accuracy : 0.8611         \n                 95% CI : (0.705, 0.9533)\n    No Information Rate : 0.3333         \n    P-Value [Acc &gt; NIR] : 8.705e-11      \n                                         \n                  Kappa : 0.7917         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.9167           0.6667\nSpecificity                 1.0000            0.8333           0.9583\nPos Pred Value              1.0000            0.7333           0.8889\nNeg Pred Value              1.0000            0.9524           0.8519\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.3056           0.2222\nDetection Prevalence        0.3333            0.4167           0.2500\nBalanced Accuracy           1.0000            0.8750           0.8125\n\n\nTeaching notes: explain cp (complexity parameter), minsplit, maxdepth, and how pruning works. Use printcp(fit_rpart) and plotcp(fit_rpart) to choose cp.\n\nprintcp(fit_rpart)\n\n\nClassification tree:\nrpart(formula = Species ~ ., data = train, method = \"class\", \n    control = rpart.control(cp = 0.01))\n\nVariables actually used in tree construction:\n[1] Petal.Length\n\nRoot node error: 76/114 = 0.66667\n\nn= 114 \n\n       CP nsplit rel error   xerror     xstd\n1 0.50000      0  1.000000 1.223684 0.054461\n2 0.46053      1  0.500000 0.723684 0.070201\n3 0.01000      2  0.039474 0.065789 0.028769\n\nplotcp(fit_rpart)\n\n\n\n\n\n\n\n# prune to the cp with lowest xerror or the 1-SE rule\nopt_cp &lt;- fit_rpart$cptable[which.min(fit_rpart$cptable[,\"xerror\"]), \"CP\"]\nfit_pruned &lt;- prune(fit_rpart, cp = opt_cp)\nrpart.plot(fit_pruned, type = 3, extra = 104)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#cross-validation-and-tuning-with-caret",
    "href": "module2.html#cross-validation-and-tuning-with-caret",
    "title": "2  Module 2 — Decision Tree Analysis & Partial Dependence",
    "section": "2.5 4. Cross-validation and tuning with caret",
    "text": "2.5 4. Cross-validation and tuning with caret\n\nctrl &lt;- trainControl(method = \"cv\", number = 5, classProbs = TRUE, summaryFunction = multiClassSummary)\nset.seed(42)\ntune_grid &lt;- expand.grid(cp = seq(0.001, 0.05, by = 0.005))\ncaret_rpart &lt;- train(Species ~ ., data = train, method = \"rpart\", trControl = ctrl, tuneGrid = tune_grid)\ncaret_rpart\n\nCART \n\n114 samples\n  4 predictor\n  3 classes: 'setosa', 'versicolor', 'virginica' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 92, 93, 90, 91, 90 \nResampling results across tuning parameters:\n\n  cp     logLoss   AUC        prAUC       Accuracy   Kappa      Mean_F1  \n  0.001  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.006  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.011  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.016  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.021  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.026  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.031  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.036  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.041  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.046  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value\n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  Mean_Precision  Mean_Recall  Mean_Detection_Rate  Mean_Balanced_Accuracy\n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.046.\n\nplot(caret_rpart)\n\n\n\n\n\n\n\n# evaluate on test\npred_caret &lt;- predict(caret_rpart, test)\nconfusionMatrix(pred_caret, test$Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         12          0         0\n  versicolor      0         11         4\n  virginica       0          1         8\n\nOverall Statistics\n                                         \n               Accuracy : 0.8611         \n                 95% CI : (0.705, 0.9533)\n    No Information Rate : 0.3333         \n    P-Value [Acc &gt; NIR] : 8.705e-11      \n                                         \n                  Kappa : 0.7917         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.9167           0.6667\nSpecificity                 1.0000            0.8333           0.9583\nPos Pred Value              1.0000            0.7333           0.8889\nNeg Pred Value              1.0000            0.9524           0.8519\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.3056           0.2222\nDetection Prevalence        0.3333            0.4167           0.2500\nBalanced Accuracy           1.0000            0.8750           0.8125\n\n\nNotes: caret::train automates CV and tuning. Explain multiClassSummary (accuracy, Kappa, etc.).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#feature-importance-interpretation",
    "href": "module2.html#feature-importance-interpretation",
    "title": "2  Module 2 — Decision Tree Analysis & Partial Dependence",
    "section": "2.6 5. Feature importance & interpretation",
    "text": "2.6 5. Feature importance & interpretation\n\n# Variable importance from rpart:\nvarImp(fit_rpart)\n\n              Overall\nPetal.Length 70.25918\nPetal.Width  70.25918\nSepal.Length 35.26587\nSepal.Width  20.46939\n\n# For caret model:\nvarImp(caret_rpart)\n\nrpart variable importance\n\n             Overall\nPetal.Width   100.00\nPetal.Length  100.00\nSepal.Length   29.72\nSepal.Width     0.00\n\n\nDiscuss how importance is computed (split improvement) and limitations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#partial-dependence-plots-pdps",
    "href": "module2.html#partial-dependence-plots-pdps",
    "title": "2  Module 2 — Decision Tree Analysis & Partial Dependence",
    "section": "2.7 6. Partial Dependence Plots (PDPs)",
    "text": "2.7 6. Partial Dependence Plots (PDPs)\nGoal: show marginal effect of a feature on predicted probability (or prediction) while averaging out other features.\n\n2.7.1 6.1 PDP with pdp\n\n# Use the caret-trained model (wrap predict function if necessary)\n# pdp works with models that have a predict method returning probabilities. We'll use the randomForest wrapper as an example.\nrf &lt;- randomForest(Species ~ ., data = train)\n\n# Partial dependence for Petal.Length vs class setosa (probability)\npdp_pl &lt;- partial(rf, pred.var = \"Petal.Length\", plot = TRUE, prob = TRUE, which.class = \"setosa\")\nprint(pdp_pl)\n\n\n\n\n\n\n\n\n\n\n2.7.2 6.2 PDP with DALEX\n\n# Create an explainer\nexplainer_rf &lt;- explain(rf, data = train[,1:4], y = train$Species, label = \"rf_iris\")\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  rf_iris \n  -&gt; data              :  114  rows  4  cols \n  -&gt; target variable   :  114  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.1 , task multiclass (  default  ) \n  -&gt; predicted values  :  predict function returns multiple columns:  3  (  default  ) \n  -&gt; residual function :  difference between 1 and probability of true class (  default  )\n  -&gt; residuals         :  numerical, min =  0 , mean =  0.01891228 , max =  0.34  \n  A new explainer has been created!  \n\n# Profile (partial dependence) using DALEX\np &lt;- model_profile(explainer_rf, variables = \"Petal.Length\", N = 50)\nplot(p)\n\n\n\n\n\n\n\n\nTeaching caveats: PDPs show average marginal effects and can be misleading with strong feature interactions. Use two-way PDPs or ICE plots for heterogeneity.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#individual-conditional-expectation-ice-and-2d-pdps",
    "href": "module2.html#individual-conditional-expectation-ice-and-2d-pdps",
    "title": "2  Module 2 — Decision Tree Analysis & Partial Dependence",
    "section": "2.8 7. Individual Conditional Expectation (ICE) and 2D PDPs",
    "text": "2.8 7. Individual Conditional Expectation (ICE) and 2D PDPs\n\n# ICE using pdp\nice_pl &lt;- partial(rf, pred.var = \"Petal.Length\", ice = TRUE, plot = TRUE, which.class = \"setosa\")\n\n# 2D PDP for Petal.Length and Petal.Width\npdp_2d &lt;- partial(rf, pred.var = c(\"Petal.Length\", \"Petal.Width\"), chull = TRUE)\nplotPartial(pdp_2d)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#model-comparison-decision-tree-vs-random-forest",
    "href": "module2.html#model-comparison-decision-tree-vs-random-forest",
    "title": "2  Module 2 — Decision Tree Analysis & Partial Dependence",
    "section": "2.9 8. Model comparison: Decision Tree vs Random Forest",
    "text": "2.9 8. Model comparison: Decision Tree vs Random Forest\n\n# Fit Random Forest and compare\nrf_fit &lt;- randomForest(Species ~ ., data = train)\npred_rf &lt;- predict(rf_fit, test)\ncm_rf &lt;- confusionMatrix(pred_rf, test$Species)\ncm_tab &lt;- rbind(tree = cm$overall[names(cm$overall) == \"Accuracy\"], rf = cm_rf$overall[names(cm_rf$overall) == \"Accuracy\"])\ncm_tab\n\n      Accuracy\ntree 0.8611111\nrf   0.9444444\n\n\nDiscuss trade-offs: interpretability (tree) vs performance (RF), stability, and overfitting.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#automated-tests-for-classroom-sanity-checks",
    "href": "module2.html#automated-tests-for-classroom-sanity-checks",
    "title": "2  Module 2 — Decision Tree Analysis & Partial Dependence",
    "section": "2.10 9. Automated tests for classroom (sanity checks)",
    "text": "2.10 9. Automated tests for classroom (sanity checks)\n\n# 1) Ensure tree predictions have reasonable accuracy (&gt; 0.7 on iris test)\nacc_tree &lt;- cm$overall[\"Accuracy\"]\nstopifnot(acc_tree &gt;= 0.7)\n\n# 2) PDP returns a data.frame and includes Petal.Length values\npdp_res &lt;- partial(rf, pred.var = \"Petal.Length\", prob = TRUE, which.class = \"setosa\", plot = FALSE)\nstopifnot(is.data.frame(pdp_res))\nstopifnot(\"Petal.Length\" %in% names(pdp_res))\n\nlist(tests = \"all passed\", tree_accuracy = acc_tree)\n\n$tests\n[1] \"all passed\"\n\n$tree_accuracy\n Accuracy \n0.8611111",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#exercises-and-in-class-tasks",
    "href": "module2.html#exercises-and-in-class-tasks",
    "title": "2  Module 2 — Decision Tree Analysis & Partial Dependence",
    "section": "2.11 10. Exercises and in-class tasks",
    "text": "2.11 10. Exercises and in-class tasks\nExercise A: Build and prune a decision tree on the wine or iris dataset; show how pruning affects depth and accuracy.\nExercise B: Compute PDPs for two top features in a caret-tuned model and interpret whether they are monotonic or have thresholds.\nExercise C (advanced): Use iml to compute Shapley values for a small set of observations and compare with PDP insights.\n\n\n\n\n\n\n\n\nEnd of Module 2 — Decision Tree Analysis & Partial Dependence",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  }
]