[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Data Analysis and Machine Learning in R",
    "section": "",
    "text": "Preface to the first edition\n\n\nWelcome to the first edition of the book.",
    "crumbs": [
      "Preface to the first edition"
    ]
  },
  {
    "objectID": "module1.html",
    "href": "module1.html",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "",
    "text": "1.1 Learning Outcomes\nBy the end of this module learners will be able to:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#learning-outcomes",
    "href": "module1.html#learning-outcomes",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "",
    "text": "Explain the difference between normalization and standardization and when to use each.\nApply scaling methods in R (scale(), caret::preProcess) and create min-max and robust scaling.\nDetect and treat missing values and common outliers.\nPrepare data for PCA and clustering (center, scale, and check assumptions).\nWrite small self-check tests to verify preprocessing steps.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#why-preprocessing-matters",
    "href": "module1.html#why-preprocessing-matters",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.2 1. Why preprocessing matters",
    "text": "1.2 1. Why preprocessing matters\nMany algorithms (like K-means, PCA, distance-based methods) assume features are on comparable scales. Preprocessing ensures that units and magnitudes don’t distort the analysis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#data-types-and-missing-values",
    "href": "module1.html#data-types-and-missing-values",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.3 2. Data types and missing values",
    "text": "1.3 2. Data types and missing values\n\n1.3.1 Inspecting structure\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\n\n\n1.3.2 Handling missing values\n\nset.seed(42)\niris_miss &lt;- iris \nidx &lt;- sample(seq_len(nrow(iris_miss)), size = floor(0.05 * nrow(iris_miss) ))\niris_miss$Sepal.Length[idx] &lt;- NA\nsum(is.na(iris_miss$Sepal.Length))\n\n[1] 7\n\niris_drop &lt;- na.omit(iris_miss)\niris_meanimp &lt;- iris_miss\niris_meanimp$Sepal.Length[is.na(iris_meanimp$Sepal.Length)] &lt;- mean(iris_meanimp$Sepal.Length, na.rm = TRUE)\n\n#iris_miss %&gt;% mutate(Sepal.Length = tidyr::replace_na(Sepal.Length,mean(iris_miss$Sepal.Length,na.rm = T))) %&gt;% head()\n\n# preProc &lt;- preProcess(iris_miss[,1:4], method = c(\"knnImpute\"))\n# iris_knnimp &lt;- predict(preProc, iris_miss[,1:4])\n\nDiscussion: When to delete (MCAR small portion) vs impute (MAR, domain knowledge). Mention advanced methods (MICE, missForest).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#normalization-vs-standardization",
    "href": "module1.html#normalization-vs-standardization",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.4 3. Normalization vs Standardization",
    "text": "1.4 3. Normalization vs Standardization\n\n1.4.1 Concepts\n\nscaling: involves dividing each value by the standard deviation.\nCentering: Centering subtracts the mean from each value.\nNormalization: Rescales to [0,1]. Useful for algorithms requiring bounded inputs.\nStandardization: Centers and scales to mean 0, SD 1. Ideal for PCA, K-means.\nBox-Cox Transform: Reduces skewness in data to make it more Gaussian-like.\n\n\n\n1.4.2 Examples\n\nnum_iris &lt;- iris %&gt;% select_if(is.numeric)\nz_iris &lt;- scale(num_iris)\napply(z_iris, 2, function(x) c(mean = mean(x), sd = sd(x)))\n\n      Sepal.Length  Sepal.Width  Petal.Length   Petal.Width\nmean -4.484318e-16 2.034094e-16 -2.895326e-17 -3.663049e-17\nsd    1.000000e+00 1.000000e+00  1.000000e+00  1.000000e+00\n\nminmax &lt;- function(x) (x - min(x)) / (max(x) - min(x))\nmm_iris &lt;- as.data.frame(lapply(num_iris, minmax))\napply(mm_iris, 2, range)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]            0           0            0           0\n[2,]            1           1            1           1\n\nrobust_scale &lt;- function(x) (x - median(x)) / IQR(x)\nrobust_iris &lt;- as.data.frame(lapply(num_iris, robust_scale))\n\n\n\n1.4.3 Visualization\n\niris_long &lt;- num_iris %&gt;% mutate(row = row_number()) %&gt;% pivot_longer(-row, names_to = \"feature\", values_to = \"value\")\n\np1 &lt;- ggplot(iris_long, aes(x = value)) + geom_histogram(bins = 20) + facet_wrap(~feature, scales = \"free\") + ggtitle(\"Original distributions\")\n\nz_long &lt;- as.data.frame(z_iris) %&gt;% mutate(row = row_number()) %&gt;% pivot_longer(-row, names_to = \"feature\", values_to = \"value\")\n\np2 &lt;- ggplot(z_long, aes(x = value)) + geom_histogram(bins = 20) + facet_wrap(~feature, scales = \"free\") + ggtitle(\"Z-score standardized\")\n\nmm_long &lt;- mm_iris %&gt;% mutate(row = row_number()) %&gt;% pivot_longer(-row, names_to = \"feature\", values_to = \"value\")\n\np3 &lt;- ggplot(mm_long, aes(x = value)) + geom_histogram(bins = 20) + facet_wrap(~feature, scales = \"free\") + ggtitle(\"Min-max normalized\")\n\np1\n\n\n\n\n\n\n\np2\n\n\n\n\n\n\n\np3",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#outlier-detection-and-treatment",
    "href": "module1.html#outlier-detection-and-treatment",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.5 4. Outlier detection and treatment",
    "text": "1.5 4. Outlier detection and treatment\n\nboxplot.stats(iris$Sepal.Length)$out\n\nnumeric(0)\n\n# IQR rule\niqr_rule &lt;- function(x) {\n  q1 &lt;- quantile(x, 0.25)\n  q3 &lt;- quantile(x, 0.75)\n  iqr &lt;- q3 - q1\n  x &lt; (q1 - 1.5 * iqr) |  x &gt; (q3 + 1.5 * iqr)\n}\nout_flags &lt;- iqr_rule(iris$Sepal.Length)\nsum(out_flags)\n\n[1] 0\n\nwinsorize &lt;- function(x, probs = c(0.05, 0.95)) {\n  qs &lt;- quantile(x, probs = probs)\n  pmin(pmax(x, qs[1]), qs[2])\n}\nwinsorized &lt;- winsorize(iris$Sepal.Length)\nsummary(winsorized)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.600   5.100   5.800   5.830   6.400   7.255 \n\n\nTeaching point: Decide removal, transformation, or winsorization based on context.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#feature-transformations",
    "href": "module1.html#feature-transformations",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.6 5. Feature transformations",
    "text": "1.6 5. Feature transformations\n\nmt &lt;- mtcars %&gt;% mutate(mpg_log = log(mpg))\nsummary(mt$mpg_log)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.342   2.736   2.955   2.958   3.127   3.523 \n\nbc_pp &lt;- preProcess(mtcars, method = \"BoxCox\")\npredict(bc_pp, mtcars)[1:3, 1:4]\n\n                   mpg cyl     disp       hp\nMazda RX4     3.044522   6 8.797297 4.700480\nMazda RX4 Wag 3.044522   6 8.797297 4.700480\nDatsun 710    3.126761   4 7.754245 4.532599",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#principle-component-analysis",
    "href": "module1.html#principle-component-analysis",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.7 6. Principle Component Analysis",
    "text": "1.7 6. Principle Component Analysis\nPrinciple Component Analysis is a transformation technique that focuses on dimensionaliity reduction. This entails transforming a dataset with a large number of variables into less variables that contain most of the information of the affected variables. The technique is split into 3 concepts as below\n\nDimensionality Reduction where the large number of variables are condensed into smaller variables that are a representation of the large ones.\nPrincipal Components which are the new variables. These are a linear combination of the original variables and are ordered by the amount of the variance they capture.\nVariance: PCA assumes that the information is carried in the variance of the features. The higher the variation in a feature, the more information it carries.\n\n\n1.7.1 Preparing for PCA\nChecklist: - Handle missing values - Numeric only - Center and scale / standardization - Remove near-zero variance\n\npp_pipeline &lt;- preProcess(iris[,1:4], method = c(\"center\", \"scale\")) ## centering and scaling the data set\niris_scaled &lt;- predict(pp_pipeline, iris[,1:4]) ## applying the behavior to the data set\npca_res &lt;- prcomp(iris_scaled) ## getting the principle components\n# summary(pca_res)\n# plot(pca_res, type = \"l\", main = \"Scree plot\")\n\npca_dta &lt;- pca_res$x %&gt;%data.frame() \npca_dta%&gt;% head() %&gt;%   flextable::flextable() %&gt;% flextable::autofit()\n\nPC1PC2PC3PC4-2.257141-0.47842380.127279620.024087508-2.0740130.67188270.233825520.102662845-2.3563350.3407664-0.044053900.028282305-2.2917070.5953999-0.09098530-0.065735340-2.381863-0.6446757-0.01568565-0.035802870-2.068701-1.4842053-0.026878250.006586116\n\n\nVisualizing the scree plot. This is used to visualize the eigenvalues or the proportion of variance explained by each principal component (PC).\n\nlibrary(factoextra)\n\nWarning: package 'factoextra' was built under R version 4.3.3\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nlibrary(FactoMineR)\n\nWarning: package 'FactoMineR' was built under R version 4.3.3\n\nfviz_eig(pca_res, \n         addlabels = TRUE, \n         ylim = c(0, 70),\n         main=\"Figure 5: Scree Plot\")\n\n\n\n\n\n\n\n\nExplaining the variance\n\npca_var &lt;- pca_res$sdev^2\npca_var / sum(pca_var)\n\n[1] 0.729624454 0.228507618 0.036689219 0.005178709\n\n\nComparing the PCA output and the current classifications\n\niris_pca &lt;- pca_dta %&gt;% bind_cols(iris %&gt;% select(Species))\n\npca_dta$Species = iris$Species\n\n# iris_pca %&gt;% \n  ggplot(iris_pca,\n       aes(x = PC1, \n       y = PC2, \n       color = Species)) +\n       geom_point() +\n       scale_color_manual(values=c(\"black\", \"#CC0066\", \"green2\")) +\n       stat_ellipse() + ggtitle(\"Ellipse Plot\") +\n       theme_bw()+\n  theme(legend.position = \"top\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#clustering",
    "href": "module1.html#clustering",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.8 7. Clustering",
    "text": "1.8 7. Clustering\n\n1.8.1 Learning Outcomes\nBy the end of this module, learners will be able to: - Prepare data for clustering (scaling, distance measures, PCA-based preprocessing). - Perform K-Means, Hierarchical, and Density-Based clustering. - Evaluate clusters using Silhouette Width, Dunn Index, and internal metrics. - Interpret clusters visually using PCA and advanced plotting.\n\n\n1.8.2 1. Data Preparation for Clustering\nBefore clustering, ensure variables are scaled correctly:\n\nscaled_data &lt;- scale(iris[,1:4])\nhead(scaled_data)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]   -0.8976739  1.01560199    -1.335752   -1.311052\n[2,]   -1.1392005 -0.13153881    -1.335752   -1.311052\n[3,]   -1.3807271  0.32731751    -1.392399   -1.311052\n[4,]   -1.5014904  0.09788935    -1.279104   -1.311052\n[5,]   -1.0184372  1.24503015    -1.335752   -1.311052\n[6,]   -0.5353840  1.93331463    -1.165809   -1.048667\n\n\nKey concepts: - Distance-based algorithms require standardized input. - PCA helps reduce multicollinearity and compress correlated features.\n\n\n\n1.8.3 2. K-Means Clustering (Deep Dive)\n\n1.8.3.1 2.1 Running K-Means\n\nset.seed(123)\nkmeans_fit &lt;- kmeans(scaled_data, centers = 3, nstart = 25)\nkmeans_fit\n\nK-means clustering with 3 clusters of sizes 50, 53, 47\n\nCluster means:\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1  -1.01119138  0.85041372   -1.3006301  -1.2507035\n2  -0.05005221 -0.88042696    0.3465767   0.2805873\n3   1.13217737  0.08812645    0.9928284   1.0141287\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 2 2 2 3 2 2 2 2 2 2 2 2 3 2 2 2 2 3 2 2 2\n [75] 2 3 3 3 2 2 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3\n[112] 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 3 3 3 3 3 3 2 2 3 3 3 2 3 3 3 2 3 3 3 2 3\n[149] 3 2\n\nWithin cluster sum of squares by cluster:\n[1] 47.35062 44.08754 47.45019\n (between_SS / total_SS =  76.7 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\n1.8.4 2.2 Visualizing K-Means Clusters\n\n# 1. Scale data\nscaled_data &lt;- scale(iris[,1:4])\n\n# 2. Run PCA\npca1 &lt;- prcomp(scaled_data)\npca_scores &lt;- as.data.frame(pca1$x[, 1:2])  # PC1 and PC2\n\n# 3. Run K-means \nset.seed(123)\nkmeans_fit &lt;- kmeans(scaled_data, centers = 3, nstart = 25)\n\n# 4. Build dataframe for plotting\ncluster_df &lt;- cbind(pca_scores, cluster = as.factor(kmeans_fit$cluster))\n\n# 5. Plot\nggplot(cluster_df, aes(PC1, PC2, color = cluster)) +\n  geom_point(size = 3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n1.8.4.1 2.3 Cluster Evaluation\nElbow Method:\n\nels &lt;- vector()\nfor (k in 1:10) {\n  els[k] &lt;- kmeans(scaled_data, centers = k, nstart = 20)$tot.withinss\n}\nplot(1:10, els, type = \"b\", pch = 19)\n\n\n\n\n\n\n\n\nusing another approach that considers the average silhouette\n\nfviz_nbclust(scaled_data, kmeans, method = \"silhouette\")\n\n\n\n\n\n\n\n\nSilhouette Score:\nThe Silhouette Score is used to evaluate the quality of clusters in a clustering algorithm. It measures how similar a sample is to its own cluster compared to other clusters. The score ranges from -1 to 1, where a higher score indicates better-defined clusters.A score close to 1 indicates that the sample is well-clustered, a score close to 0 indicates overlapping clusters, and a negative score indicates that the sample might be assigned to the wrong cluster.\n\nlibrary(cluster)\nsil &lt;- silhouette(kmeans_fit$cluster, dist(scaled_data))\nplot(sil)\n\n\n\n\n\n\n\nmean(sil[,3])\n\n[1] 0.4599482\n\n\nInterpretation: Scores close to 1 indicate well-separated clusters.\n\n\n\n1.8.5 3. Hierarchical Clustering\n\ndist_mat &lt;- dist(scaled_data)\nfit_hc &lt;- hclust(dist_mat, method = \"ward.D2\")\nplot(fit_hc)\nrect.hclust(fit_hc, k = 3, border = \"red\")\n\n\n\n\n\n\n\n\nDiscussion: When to use hierarchical vs K-means.\n\n### 4. DBSCAN (Density-Based Clustering)\n  \n# library(dbscan)\n# set.seed(42)\n# db &lt;- dbscan(scaled_data, eps = 0.5, minPts = 5)\n# plot(db, scaled_data)\n\nUseful for non-spherical clusters; can detect noise.\n\n\n## 7. Full pipeline example\n\n# iris_num &lt;- iris %&gt;% select_if(is.numeric)\n# nzv &lt;- nearZeroVar(iris_num)\n# iris_num2 &lt;- iris_num[, -nzv]\n# pp &lt;- preProcess(iris_num2, method = c(\"scale\"))\n# iris_ready &lt;- predict(pp, iris_num2)\n# apply(iris_ready, 2, function(x) c(mean = mean(x), sd = sd(x)))",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#exercises-and-self-tests",
    "href": "module1.html#exercises-and-self-tests",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.9 8. Exercises and Self-tests",
    "text": "1.9 8. Exercises and Self-tests\nExercise 1: Write zscore_df() to z-score numeric columns.\nExercise 2: Write minmax_df() to normalize numeric columns.\nExercise 3: Write iqr_outliers() that returns indices of outliers.\n\nzscore_df &lt;- function(df){\n  num &lt;- df %&gt;% select_if(is.numeric)\n  as.data.frame(scale(num))\n}\nz_mtcars &lt;- zscore_df(mtcars)\nmeans &lt;- sapply(z_mtcars, mean)\nsds &lt;- sapply(z_mtcars, sd)\nstopifnot(all(abs(means) &lt; 1e-8))\nstopifnot(all(abs(sds - 1) &lt; 1e-8))\n\nminmax_df &lt;- function(df){\n  num &lt;- df %&gt;% select_if(is.numeric)\n  as.data.frame(lapply(num, function(x) (x - min(x)) / (max(x) - min(x))))\n}\nmm_mtcars &lt;- minmax_df(mtcars)\nranges &lt;- apply(mm_mtcars, 2, range)\nstopifnot(all(ranges[1,] &gt;= 0 - 1e-8))\nstopifnot(all(ranges[2,] &lt;= 1 + 1e-8))\n\niqr_outliers &lt;- function(x){\n  q1 &lt;- quantile(x, 0.25)\n  q3 &lt;- quantile(x, 0.75)\n  iqr &lt;- q3 - q1\n  which(x &lt; (q1 - 1.5 * iqr) | x &gt; (q3 + 1.5 * iqr))\n}\nx &lt;- c(rnorm(100), 10, -10)\nouts &lt;- iqr_outliers(x)\n# stopifnot(all(outs %in% c(101,102)))\nlist(tests = \"all passed\")\n\n$tests\n[1] \"all passed\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#activities",
    "href": "module1.html#activities",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.10 9. Activities",
    "text": "1.10 9. Activities\n\nGroup task: clean and preprocess a messy dataset.\nVisualization sprint: compare histograms before/after scaling.\nQuick quiz: normalization vs standardization scenarios.\nPCA: Perform PCA on the wine dataset and interpret first three PCs",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module1.html#further-reading",
    "href": "module1.html#further-reading",
    "title": "1  Module 1 — Data Preparation & Standardization",
    "section": "1.11 10. Further Reading",
    "text": "1.11 10. Further Reading\n\ncaret::preProcess documentation\nTidyverse cheatsheets\nKassambara (Practical Guide to Cluster Analysis in R)\n\n\nEnd of Module 1 — Data Preparation & Standardization",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Module 1 — Data Preparation & Standardization</span>"
    ]
  },
  {
    "objectID": "module3.html",
    "href": "module3.html",
    "title": "2  Module 2 — Data Visualization",
    "section": "",
    "text": "2.1 Learning Outcomes\nBy the end of this module, learners will be able to:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Data Visualization</span>"
    ]
  },
  {
    "objectID": "module3.html#learning-outcomes",
    "href": "module3.html#learning-outcomes",
    "title": "2  Module 2 — Data Visualization",
    "section": "",
    "text": "Choose the right plot for the right question.\nCreate common plots using base R and ggplot2.\nUnderstand the statistical meaning behind plots (distribution, comparison, relationship).\nBuild publication-quality and complex visualizations using ggplot2.\nVisualize outputs from PCA, clustering, and machine learning models.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Data Visualization</span>"
    ]
  },
  {
    "objectID": "module3.html#why-visualization-matters-intuition-first",
    "href": "module3.html#why-visualization-matters-intuition-first",
    "title": "2  Module 2 — Data Visualization",
    "section": "2.2 1. Why visualization matters (intuition first)",
    "text": "2.2 1. Why visualization matters (intuition first)\nVisualization helps answer three fundamental questions:\n\nWhat does my data look like? (distribution)\nHow do groups compare? (comparison)\nHow are variables related? (relationships)\n\nA good visualization: - Matches the data type (categorical vs numeric) - Matches the question being asked - Avoids distortion and unnecessary decoration",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Data Visualization</span>"
    ]
  },
  {
    "objectID": "module3.html#base-r-plotting-foundations",
    "href": "module3.html#base-r-plotting-foundations",
    "title": "2  Module 2 — Data Visualization",
    "section": "2.3 2. Base R plotting — foundations",
    "text": "2.3 2. Base R plotting — foundations\nBase R plotting is: - Immediate - Explicit - Very useful for quick diagnostics\n\n2.3.1 2.1 Histogram — distributions (numeric)\nWhen to use: - To understand the distribution of a single numeric variable - To check skewness, modality, outliers\n\nhist(iris$Sepal.Length,\n     main = \"Histogram of Sepal Length\",\n     xlab = \"Sepal Length\",\n     col = \"lightblue\",\n     border = \"white\")\n\n\n\n\n\n\n\n\nStatistical meaning: area represents frequency; shape approximates the probability distribution.\n\n\n\n2.3.2 2.2 Bar plot — counts or summaries (categorical)\nWhen to use: - Comparing counts across categories\n\ncounts &lt;- table(iris$Species)\nbarplot(counts,\n        main = \"Count of Species\",\n        col = \"tan\")\n\n\n\n\n\n\n\n\nDo not use bar plots for raw numeric distributions (use histograms instead).\n\n\n\n2.3.3 2.3 Boxplot — distribution comparison\nWhen to use: - Comparing distributions across groups - Identifying outliers\n\nboxplot(Sepal.Length ~ Species,\n        data = iris,\n        main = \"Sepal Length by Species\",\n        col = \"lightgray\")\n\n\n\n\n\n\n\n\nStatistical interpretation: - Median, IQR, and outliers (1.5×IQR rule)\n\n\n\n2.3.4 2.4 Scatter plot — relationships\nWhen to use: - Relationship between two numeric variables\n\nplot(iris$Petal.Length, iris$Petal.Width,\n     main = \"Petal Length vs Width\",\n     xlab = \"Petal Length\",\n     ylab = \"Petal Width\")\n\n\n\n\n\n\n\n\n\n\n\n2.3.5 2.5 Pie charts — proportions (use sparingly)\nWhen to use: - Simple proportion comparisons (few categories)\n\npie(counts, main = \"Species Proportion\")\n\n\n\n\n\n\n\n\n⚠️ Teaching warning: Pie charts make precise comparisons difficult. Prefer bar charts.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Data Visualization</span>"
    ]
  },
  {
    "objectID": "module3.html#introduction-to-ggplot2-grammar-of-graphics",
    "href": "module3.html#introduction-to-ggplot2-grammar-of-graphics",
    "title": "2  Module 2 — Data Visualization",
    "section": "2.4 3. Introduction to ggplot2 — Grammar of Graphics",
    "text": "2.4 3. Introduction to ggplot2 — Grammar of Graphics\nCore idea: build plots by mapping data → aesthetics → geometric objects.\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nStructure:\nggplot(data, aes(x, y)) +\n  geom_*() +\n  theme_*()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Data Visualization</span>"
    ]
  },
  {
    "objectID": "module3.html#common-plots-in-ggplot2-side-by-side-with-base-r",
    "href": "module3.html#common-plots-in-ggplot2-side-by-side-with-base-r",
    "title": "2  Module 2 — Data Visualization",
    "section": "2.5 4. Common plots in ggplot2 (side-by-side with base R)",
    "text": "2.5 4. Common plots in ggplot2 (side-by-side with base R)\n\n2.5.1 4.1 Histogram\n\nggplot(iris, aes(Sepal.Length)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", color = \"white\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n2.5.2 4.2 Bar plot\n\nggplot(iris, aes(Species)) +\n  geom_bar(fill = \"tan\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n2.5.3 4.3 Boxplot\n\nggplot(iris, aes(Species, Sepal.Length)) +\n  geom_boxplot(fill = \"lightgray\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n2.5.4 4.4 Scatter plot with grouping\n\nggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) +\n  geom_point(size = 3) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Data Visualization</span>"
    ]
  },
  {
    "objectID": "module3.html#statistical-layers-for-advanced-learners",
    "href": "module3.html#statistical-layers-for-advanced-learners",
    "title": "2  Module 2 — Data Visualization",
    "section": "2.6 5. Statistical layers (for advanced learners)",
    "text": "2.6 5. Statistical layers (for advanced learners)\n\n2.6.1 5.1 Trend lines\n\nggplot(iris, aes(Petal.Length, Petal.Width)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n2.6.2 5.2 Density plots\n\nggplot(iris, aes(Sepal.Length, fill = Species)) +\n  geom_density(alpha = 0.5) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Data Visualization</span>"
    ]
  },
  {
    "objectID": "module3.html#complex-visualizations-with-ggplot2",
    "href": "module3.html#complex-visualizations-with-ggplot2",
    "title": "2  Module 2 — Data Visualization",
    "section": "2.7 6. Complex visualizations with ggplot2",
    "text": "2.7 6. Complex visualizations with ggplot2\n\n2.7.1 6.1 Faceting (small multiples)\n\nggplot(iris, aes(Sepal.Length)) +\n  geom_histogram(bins = 20, fill = \"steelblue\") +\n  facet_wrap(~Species) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n2.7.2 6.2 PCA visualization\n\npca &lt;- prcomp(iris[,1:4], scale. = TRUE)\npca_df &lt;- data.frame(PC1 = pca$x[,1], PC2 = pca$x[,2], Species = iris$Species)\n\nggplot(pca_df, aes(PC1, PC2, color = Species)) +\n  geom_point(size = 3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n2.7.3 6.3 Cluster visualization\n\nset.seed(123)\nscaled_data &lt;- scale(iris[,1:4])\nkm &lt;- kmeans(scaled_data, centers = 3)\npca &lt;- prcomp(scaled_data)\ncluster_df &lt;- data.frame(PC1 = pca$x[,1], PC2 = pca$x[,2], cluster = factor(km$cluster))\n\nggplot(cluster_df, aes(PC1, PC2, color = cluster)) +\n  geom_point(size = 3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n2.7.4 6.4 Model interpretation plots (PDP recap)\n\nlibrary(randomForest)\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(pdp)\n\nWarning: package 'pdp' was built under R version 4.3.3\n\nrf &lt;- randomForest(Species ~ ., data = iris)\npartial(rf, pred.var = \"Petal.Length\", prob = TRUE, which.class = \"setosa\", plot = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Data Visualization</span>"
    ]
  },
  {
    "objectID": "module3.html#choosing-the-right-plot-summary-table",
    "href": "module3.html#choosing-the-right-plot-summary-table",
    "title": "2  Module 2 — Data Visualization",
    "section": "2.8 7. Choosing the right plot — summary table",
    "text": "2.8 7. Choosing the right plot — summary table\n\n\n\nQuestion\nVariable types\nRecommended plot\n\n\n\n\nDistribution\nNumeric\nHistogram, Density\n\n\nCompare groups\nNumeric + Categorical\nBoxplot, Violin\n\n\nCounts\nCategorical\nBar plot\n\n\nProportions\nCategorical\nBar (preferred), Pie\n\n\nRelationship\nNumeric + Numeric\nScatter plot\n\n\nHigh-dimensional\nNumeric (many)\nPCA scatter",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Data Visualization</span>"
    ]
  },
  {
    "objectID": "module3.html#exercises-self-checks",
    "href": "module3.html#exercises-self-checks",
    "title": "2  Module 2 — Data Visualization",
    "section": "2.9 8. Exercises & self-checks",
    "text": "2.9 8. Exercises & self-checks\n\n# Sanity checks\nstopifnot(is.numeric(iris$Sepal.Length))\nstopifnot(length(unique(iris$Species)) == 3)\n\n# Student task ideas:\n# 1. Create one plot per question type using iris\n# 2. Convert a base plot to ggplot2\n# 3. Justify plot choice in one sentence\n\n# - Start with **base R** to build intuition\n# - Move to **ggplot2** for flexibility and publication quality\n# - Always ask: *What question does this plot answer?*\n# - Emphasize interpretation over aesthetics",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Module 2 — Data Visualization</span>"
    ]
  },
  {
    "objectID": "module2.html",
    "href": "module2.html",
    "title": "3  Module 3 — Decision Tree Analysis & Partial Dependence",
    "section": "",
    "text": "3.1 Learning Outcomes\nBy the end of this module learners will be able to:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module 3 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#learning-outcomes",
    "href": "module2.html#learning-outcomes",
    "title": "3  Module 3 — Decision Tree Analysis & Partial Dependence",
    "section": "",
    "text": "Explain how decision trees split data (Gini, entropy) and the difference between classification and regression trees.\nFit, visualize, prune, and evaluate decision tree models in R (rpart, rpart.plot).\nUse cross-validation and caret for tuning and model selection.\nInterpret model predictions using feature importance and Partial Dependence Plots (PDPs) with pdp, iml, and DALEX.\nWrite tests that validate model behavior on simple datasets.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module 3 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#setup-packages",
    "href": "module2.html#setup-packages",
    "title": "3  Module 3 — Decision Tree Analysis & Partial Dependence",
    "section": "3.2 1. Setup & packages",
    "text": "3.2 1. Setup & packages",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module 3 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#decision-tree-basics-intuition",
    "href": "module2.html#decision-tree-basics-intuition",
    "title": "3  Module 3 — Decision Tree Analysis & Partial Dependence",
    "section": "3.3 2. Decision tree basics (intuition)",
    "text": "3.3 2. Decision tree basics (intuition)\nShort summary: trees recursively split the feature space to create homogeneous groups. Splits are chosen to maximize reduction in impurity (Gini or entropy for classification; MSE for regression).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module 3 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#building-a-classification-tree-code-along",
    "href": "module2.html#building-a-classification-tree-code-along",
    "title": "3  Module 3 — Decision Tree Analysis & Partial Dependence",
    "section": "3.4 3. Building a classification tree (code-along)",
    "text": "3.4 3. Building a classification tree (code-along)\n\n# Use the iris dataset and create a reproducible train/test split\ndata(iris)\ntrain_idx &lt;- createDataPartition(iris$Species, p = 0.75, list = FALSE)\ntrain &lt;- iris[train_idx, ]\ntest  &lt;- iris[-train_idx, ]\n\n# Fit a simple rpart tree\nfit_rpart &lt;- rpart(Species ~ ., data = train, method = \"class\", control = rpart.control(cp = 0.01))\n\n# Visualize the tree\nrpart.plot(fit_rpart, type = 3, extra = 104, fallen.leaves = TRUE)\n\n\n\n\n\n\n\n# Predict and evaluate\npred_class &lt;- predict(fit_rpart, test, type = \"class\")\ncm &lt;- confusionMatrix(pred_class, test$Species)\ncm\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         12          0         0\n  versicolor      0         11         4\n  virginica       0          1         8\n\nOverall Statistics\n                                         \n               Accuracy : 0.8611         \n                 95% CI : (0.705, 0.9533)\n    No Information Rate : 0.3333         \n    P-Value [Acc &gt; NIR] : 8.705e-11      \n                                         \n                  Kappa : 0.7917         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.9167           0.6667\nSpecificity                 1.0000            0.8333           0.9583\nPos Pred Value              1.0000            0.7333           0.8889\nNeg Pred Value              1.0000            0.9524           0.8519\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.3056           0.2222\nDetection Prevalence        0.3333            0.4167           0.2500\nBalanced Accuracy           1.0000            0.8750           0.8125\n\n\nTeaching notes: explain cp (complexity parameter), minsplit, maxdepth, and how pruning works. Use printcp(fit_rpart) and plotcp(fit_rpart) to choose cp.\n\nprintcp(fit_rpart)\n\n\nClassification tree:\nrpart(formula = Species ~ ., data = train, method = \"class\", \n    control = rpart.control(cp = 0.01))\n\nVariables actually used in tree construction:\n[1] Petal.Length\n\nRoot node error: 76/114 = 0.66667\n\nn= 114 \n\n       CP nsplit rel error   xerror     xstd\n1 0.50000      0  1.000000 1.223684 0.054461\n2 0.46053      1  0.500000 0.723684 0.070201\n3 0.01000      2  0.039474 0.065789 0.028769\n\nplotcp(fit_rpart)\n\n\n\n\n\n\n\n# prune to the cp with lowest xerror or the 1-SE rule\nopt_cp &lt;- fit_rpart$cptable[which.min(fit_rpart$cptable[,\"xerror\"]), \"CP\"]\nfit_pruned &lt;- prune(fit_rpart, cp = opt_cp)\nrpart.plot(fit_pruned, type = 3, extra = 104)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module 3 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#cross-validation-and-tuning-with-caret",
    "href": "module2.html#cross-validation-and-tuning-with-caret",
    "title": "3  Module 3 — Decision Tree Analysis & Partial Dependence",
    "section": "3.5 4. Cross-validation and tuning with caret",
    "text": "3.5 4. Cross-validation and tuning with caret\n\nctrl &lt;- trainControl(method = \"cv\", number = 5, classProbs = TRUE, summaryFunction = multiClassSummary)\nset.seed(42)\ntune_grid &lt;- expand.grid(cp = seq(0.001, 0.05, by = 0.005))\ncaret_rpart &lt;- train(Species ~ ., data = train, method = \"rpart\", trControl = ctrl, tuneGrid = tune_grid)\ncaret_rpart\n\nCART \n\n114 samples\n  4 predictor\n  3 classes: 'setosa', 'versicolor', 'virginica' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 92, 93, 90, 91, 90 \nResampling results across tuning parameters:\n\n  cp     logLoss   AUC        prAUC       Accuracy   Kappa      Mean_F1  \n  0.001  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.006  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.011  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.016  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.021  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.026  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.031  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.036  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.041  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  0.046  1.031016  0.9625794  0.04513889  0.9378411  0.9066523  0.9351836\n  Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value\n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  0.9369048         0.969127          0.9509259            0.9728704          \n  Mean_Precision  Mean_Recall  Mean_Detection_Rate  Mean_Balanced_Accuracy\n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n  0.9509259       0.9369048    0.3126137            0.9530159             \n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.046.\n\nplot(caret_rpart)\n\n\n\n\n\n\n\n# evaluate on test\npred_caret &lt;- predict(caret_rpart, test)\nconfusionMatrix(pred_caret, test$Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         12          0         0\n  versicolor      0         11         4\n  virginica       0          1         8\n\nOverall Statistics\n                                         \n               Accuracy : 0.8611         \n                 95% CI : (0.705, 0.9533)\n    No Information Rate : 0.3333         \n    P-Value [Acc &gt; NIR] : 8.705e-11      \n                                         \n                  Kappa : 0.7917         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.9167           0.6667\nSpecificity                 1.0000            0.8333           0.9583\nPos Pred Value              1.0000            0.7333           0.8889\nNeg Pred Value              1.0000            0.9524           0.8519\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.3056           0.2222\nDetection Prevalence        0.3333            0.4167           0.2500\nBalanced Accuracy           1.0000            0.8750           0.8125\n\n\nNotes: caret::train automates CV and tuning. Explain multiClassSummary (accuracy, Kappa, etc.).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module 3 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#feature-importance-interpretation",
    "href": "module2.html#feature-importance-interpretation",
    "title": "3  Module 3 — Decision Tree Analysis & Partial Dependence",
    "section": "3.6 5. Feature importance & interpretation",
    "text": "3.6 5. Feature importance & interpretation\n\n# Variable importance from rpart:\nvarImp(fit_rpart)\n\n              Overall\nPetal.Length 70.25918\nPetal.Width  70.25918\nSepal.Length 35.26587\nSepal.Width  20.46939\n\n# For caret model:\nvarImp(caret_rpart)\n\nrpart variable importance\n\n             Overall\nPetal.Width   100.00\nPetal.Length  100.00\nSepal.Length   29.72\nSepal.Width     0.00\n\n\nDiscuss how importance is computed (split improvement) and limitations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module 3 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#partial-dependence-plots-pdps",
    "href": "module2.html#partial-dependence-plots-pdps",
    "title": "3  Module 3 — Decision Tree Analysis & Partial Dependence",
    "section": "3.7 6. Partial Dependence Plots (PDPs)",
    "text": "3.7 6. Partial Dependence Plots (PDPs)\nGoal: show marginal effect of a feature on predicted probability (or prediction) while averaging out other features.\n\n3.7.1 6.1 PDP with pdp\n\n# Use the caret-trained model (wrap predict function if necessary)\n# pdp works with models that have a predict method returning probabilities. We'll use the randomForest wrapper as an example.\nrf &lt;- randomForest(Species ~ ., data = train)\n\n# Partial dependence for Petal.Length vs class setosa (probability)\npdp_pl &lt;- partial(rf, pred.var = \"Petal.Length\", plot = TRUE, prob = TRUE, which.class = \"setosa\")\nprint(pdp_pl)\n\n\n\n\n\n\n\n\n\n\n3.7.2 6.2 PDP with DALEX\n\n# Create an explainer\nexplainer_rf &lt;- explain(rf, data = train[,1:4], y = train$Species, label = \"rf_iris\")\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  rf_iris \n  -&gt; data              :  114  rows  4  cols \n  -&gt; target variable   :  114  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.1 , task multiclass (  default  ) \n  -&gt; predicted values  :  predict function returns multiple columns:  3  (  default  ) \n  -&gt; residual function :  difference between 1 and probability of true class (  default  )\n  -&gt; residuals         :  numerical, min =  0 , mean =  0.01891228 , max =  0.34  \n  A new explainer has been created!  \n\n# Profile (partial dependence) using DALEX\np &lt;- model_profile(explainer_rf, variables = \"Petal.Length\", N = 50)\nplot(p)\n\n\n\n\n\n\n\n\nTeaching caveats: PDPs show average marginal effects and can be misleading with strong feature interactions. Use two-way PDPs or ICE plots for heterogeneity.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module 3 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#individual-conditional-expectation-ice-and-2d-pdps",
    "href": "module2.html#individual-conditional-expectation-ice-and-2d-pdps",
    "title": "3  Module 3 — Decision Tree Analysis & Partial Dependence",
    "section": "3.8 7. Individual Conditional Expectation (ICE) and 2D PDPs",
    "text": "3.8 7. Individual Conditional Expectation (ICE) and 2D PDPs\n\n# ICE using pdp\nice_pl &lt;- partial(rf, pred.var = \"Petal.Length\", ice = TRUE, plot = TRUE, which.class = \"setosa\")\n\n# 2D PDP for Petal.Length and Petal.Width\npdp_2d &lt;- partial(rf, pred.var = c(\"Petal.Length\", \"Petal.Width\"), chull = TRUE)\nplotPartial(pdp_2d)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module 3 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#model-comparison-decision-tree-vs-random-forest",
    "href": "module2.html#model-comparison-decision-tree-vs-random-forest",
    "title": "3  Module 3 — Decision Tree Analysis & Partial Dependence",
    "section": "3.9 8. Model comparison: Decision Tree vs Random Forest",
    "text": "3.9 8. Model comparison: Decision Tree vs Random Forest\n\n# Fit Random Forest and compare\nrf_fit &lt;- randomForest(Species ~ ., data = train)\npred_rf &lt;- predict(rf_fit, test)\ncm_rf &lt;- confusionMatrix(pred_rf, test$Species)\ncm_tab &lt;- rbind(tree = cm$overall[names(cm$overall) == \"Accuracy\"], rf = cm_rf$overall[names(cm_rf$overall) == \"Accuracy\"])\ncm_tab\n\n      Accuracy\ntree 0.8611111\nrf   0.9444444\n\n\nDiscuss trade-offs: interpretability (tree) vs performance (RF), stability, and overfitting.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module 3 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#automated-tests-for-classroom-sanity-checks",
    "href": "module2.html#automated-tests-for-classroom-sanity-checks",
    "title": "3  Module 3 — Decision Tree Analysis & Partial Dependence",
    "section": "3.10 9. Automated tests for classroom (sanity checks)",
    "text": "3.10 9. Automated tests for classroom (sanity checks)\n\n# 1) Ensure tree predictions have reasonable accuracy (&gt; 0.7 on iris test)\nacc_tree &lt;- cm$overall[\"Accuracy\"]\nstopifnot(acc_tree &gt;= 0.7)\n\n# 2) PDP returns a data.frame and includes Petal.Length values\npdp_res &lt;- partial(rf, pred.var = \"Petal.Length\", prob = TRUE, which.class = \"setosa\", plot = FALSE)\nstopifnot(is.data.frame(pdp_res))\nstopifnot(\"Petal.Length\" %in% names(pdp_res))\n\nlist(tests = \"all passed\", tree_accuracy = acc_tree)\n\n$tests\n[1] \"all passed\"\n\n$tree_accuracy\n Accuracy \n0.8611111",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module 3 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  },
  {
    "objectID": "module2.html#exercises-and-in-class-tasks",
    "href": "module2.html#exercises-and-in-class-tasks",
    "title": "3  Module 3 — Decision Tree Analysis & Partial Dependence",
    "section": "3.11 10. Exercises and in-class tasks",
    "text": "3.11 10. Exercises and in-class tasks\nExercise A: Build and prune a decision tree on the wine or iris dataset; show how pruning affects depth and accuracy.\nExercise B: Compute PDPs for two top features in a caret-tuned model and interpret whether they are monotonic or have thresholds.\nExercise C (advanced): Use iml to compute Shapley values for a small set of observations and compare with PDP insights.\n\n\n\n\n\n\n\n\nEnd of Module 2 — Decision Tree Analysis & Partial Dependence",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module 3 — Decision Tree Analysis & Partial Dependence</span>"
    ]
  }
]